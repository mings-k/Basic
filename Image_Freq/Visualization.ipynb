{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습한 pth 파일을 이용한 Grad-CAM 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import matplotlib.pyplot as plt \n",
    "import sys\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        #residual function\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        #shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        #the shortcut output dimension is not the same with residual function\n",
    "        #use 1*1 convolution to match the dimension\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "# 222222\n",
    "class L_BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #BasicBlock and BottleNeck block\n",
    "    #have different output size\n",
    "    #we use class attribute expansion\n",
    "    #to distinct\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        #residual function\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * L_BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * L_BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        #shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        #the shortcut output dimension is not the same with residual function\n",
    "        #use 1*1 convolution to match the dimension\n",
    "        if stride != 1 or in_channels != L_BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * L_BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * L_BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = x.clone().detach()\n",
    "        x_fft = torch.fft.fft2(x, dim=(2, 3))\n",
    "        x_fft_shifted = torch.fft.fftshift(x_fft, dim=(2, 3))\n",
    "\n",
    "        rows, cols = x_fft_shifted.shape[2], x_fft_shifted.shape[3]\n",
    "        center_row, center_col = rows // 2, cols // 2\n",
    "        cutoff_frequency = min(rows, cols) // 3\n",
    "\n",
    "        high_pass_mask = torch.ones_like(x_fft_shifted, dtype=torch.float32)\n",
    "        low_pass_mask = torch.zeros_like(x_fft_shifted, dtype=torch.float32)\n",
    "        \n",
    "        y, x = torch.meshgrid(torch.arange(rows), torch.arange(cols), indexing='ij')\n",
    "        dist = torch.sqrt((x - center_col)**2 + (y - center_row)**2)\n",
    "\n",
    "        high_pass_mask[:, :, dist <= cutoff_frequency] = 0\n",
    "        low_pass_mask[:, :, dist <= cutoff_frequency] = 1\n",
    "\n",
    "        high_pass = x_fft_shifted * high_pass_mask\n",
    "        low_pass = x_fft_shifted * low_pass_mask\n",
    "\n",
    "        high_pass_ifft = torch.fft.ifft2(torch.fft.ifftshift(high_pass, dim=(2, 3)), dim=(2, 3)).real\n",
    "        low_pass_ifft = torch.fft.ifft2(torch.fft.ifftshift(low_pass, dim=(2, 3)), dim=(2, 3)).real\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x_1) + self.shortcut(low_pass_ifft))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class L_BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * L_BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * L_BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * L_BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * L_BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * L_BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = x.clone().detach()\n",
    "        x_fft = torch.fft.fft2(x, dim=(2, 3))\n",
    "        x_fft_shifted = torch.fft.fftshift(x_fft, dim=(2, 3))\n",
    "\n",
    "        rows, cols = x_fft_shifted.shape[2], x_fft_shifted.shape[3]\n",
    "        center_row, center_col = rows // 2, cols // 2\n",
    "        cutoff_frequency = min(rows, cols) // 3\n",
    "\n",
    "        high_pass_mask = torch.ones_like(x_fft_shifted, dtype=torch.float32)\n",
    "        low_pass_mask = torch.zeros_like(x_fft_shifted, dtype=torch.float32)\n",
    "        \n",
    "        y, x = torch.meshgrid(torch.arange(rows), torch.arange(cols), indexing='ij')\n",
    "        dist = torch.sqrt((x - center_col)**2 + (y - center_row)**2)\n",
    "\n",
    "        high_pass_mask[:, :, dist <= cutoff_frequency] = 0\n",
    "        low_pass_mask[:, :, dist <= cutoff_frequency] = 1\n",
    "\n",
    "        high_pass = x_fft_shifted * high_pass_mask\n",
    "        low_pass = x_fft_shifted * low_pass_mask\n",
    "\n",
    "        high_pass_ifft = torch.fft.ifft2(torch.fft.ifftshift(high_pass, dim=(2, 3)), dim=(2, 3)).real\n",
    "        low_pass_ifft = torch.fft.ifft2(torch.fft.ifftshift(low_pass, dim=(2, 3)), dim=(2, 3)).real\n",
    "        \n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x_1) + self.shortcut(low_pass_ifft))\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block,L_BasicBlock, num_block, num_classes=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        #we use a different inputsize than the original paper\n",
    "        #so conv2_x's stride is 1\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(L_BasicBlock, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(L_BasicBlock, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        output = self.conv3_x(output)\n",
    "        output = self.conv4_x(output)\n",
    "        output = self.conv5_x(output)\n",
    "        output = self.avg_pool(output)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "def resnet18():\n",
    "    \"\"\" return a ResNet 18 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock,L_BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "def resnet34():\n",
    "    \"\"\" return a ResNet 34 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock,L_BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def resnet50():\n",
    "    \"\"\" return a ResNet 50 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck,L_BottleNeck, [3, 4, 6, 3])\n",
    "\n",
    "def resnet101():\n",
    "    \"\"\" return a ResNet 101 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck,L_BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "def resnet152():\n",
    "    \"\"\" return a ResNet 152 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, L_BottleNeck, [3, 8, 36, 3])\n",
    "\n",
    "\n",
    "# 이미지 전처리 함수 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "# GPU 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "net = resnet18().to(device)\n",
    "net.load_state_dict((torch.load(\"o_BLLB.pth\")))\n",
    "# net.load_state_dict((torch.load(\"original.pth\")))\n",
    "net.eval()\n",
    "\n",
    "\n",
    "data_iter = iter(testloader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# 배치 중 첫 번째 이미지를 선택합니다.\n",
    "image = images[57].unsqueeze(0).to(device)  # 배치 차원 추가\n",
    "label = labels[57].item()  # 라벨을 정수로 변환\n",
    "\n",
    "target_layers1 = [net.conv2_x[-1]]\n",
    "target_layers2 = [net.conv3_x[-1]]\n",
    "target_layers3 = [net.conv4_x[-1]]\n",
    "target_layers4 = [net.conv5_x[-1]]\n",
    "\n",
    "targets = [ClassifierOutputTarget(label)]\n",
    "\n",
    "def for_cam(target_layers1, target_layers2, target_layers3, target_layers4, image, targets):\n",
    "    target_layers_list = [target_layers1, target_layers2, target_layers3, target_layers4]\n",
    "    num_layers = len(target_layers_list)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 원본 이미지 시각화\n",
    "    rgb_img = image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    plt.subplot(1, num_layers + 1, 1)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    for i, target_layers in enumerate(target_layers_list):\n",
    "        cam = GradCAM(model=net, target_layers=target_layers, use_cuda=torch.cuda.is_available())\n",
    "        grayscale_cam = cam(input_tensor=image, targets=targets)\n",
    "        grayscale_cam = grayscale_cam[0, :]  # 배치 차원 제거\n",
    "\n",
    "        # CAM을 원본 이미지에 오버레이\n",
    "        visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "        \n",
    "        plt.subplot(1, num_layers + 1, i + 2)\n",
    "        plt.imshow(visualization)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for_cam(target_layers1, target_layers2, target_layers3, target_layers4, image, targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #### 학습한 pth 파일을 이용한 Feature Map 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import matplotlib.pyplot as plt \n",
    "import sys\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        #residual function\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        #shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        #the shortcut output dimension is not the same with residual function\n",
    "        #use 1*1 convolution to match the dimension\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "# 222222\n",
    "class L_BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #BasicBlock and BottleNeck block\n",
    "    #have different output size\n",
    "    #we use class attribute expansion\n",
    "    #to distinct\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        #residual function\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * L_BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * L_BasicBlock.expansion)\n",
    "        )\n",
    "\n",
    "        #shortcut\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        #the shortcut output dimension is not the same with residual function\n",
    "        #use 1*1 convolution to match the dimension\n",
    "        if stride != 1 or in_channels != L_BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * L_BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * L_BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = x.clone().detach()\n",
    "        x_fft = torch.fft.fft2(x, dim=(2, 3))\n",
    "        x_fft_shifted = torch.fft.fftshift(x_fft, dim=(2, 3))\n",
    "\n",
    "        rows, cols = x_fft_shifted.shape[2], x_fft_shifted.shape[3]\n",
    "        center_row, center_col = rows // 2, cols // 2\n",
    "        cutoff_frequency = min(rows, cols) // 3\n",
    "\n",
    "        high_pass_mask = torch.ones_like(x_fft_shifted, dtype=torch.float32)\n",
    "        low_pass_mask = torch.zeros_like(x_fft_shifted, dtype=torch.float32)\n",
    "        \n",
    "        y, x = torch.meshgrid(torch.arange(rows), torch.arange(cols), indexing='ij')\n",
    "        dist = torch.sqrt((x - center_col)**2 + (y - center_row)**2)\n",
    "\n",
    "        high_pass_mask[:, :, dist <= cutoff_frequency] = 0\n",
    "        low_pass_mask[:, :, dist <= cutoff_frequency] = 1\n",
    "\n",
    "        high_pass = x_fft_shifted * high_pass_mask\n",
    "        low_pass = x_fft_shifted * low_pass_mask\n",
    "\n",
    "        high_pass_ifft = torch.fft.ifft2(torch.fft.ifftshift(high_pass, dim=(2, 3)), dim=(2, 3)).real\n",
    "        low_pass_ifft = torch.fft.ifft2(torch.fft.ifftshift(low_pass, dim=(2, 3)), dim=(2, 3)).real\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x_1) + self.shortcut(low_pass_ifft))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class L_BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * L_BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * L_BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * L_BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * L_BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * L_BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = x.clone().detach()\n",
    "        x_fft = torch.fft.fft2(x, dim=(2, 3))\n",
    "        x_fft_shifted = torch.fft.fftshift(x_fft, dim=(2, 3))\n",
    "\n",
    "        rows, cols = x_fft_shifted.shape[2], x_fft_shifted.shape[3]\n",
    "        center_row, center_col = rows // 2, cols // 2\n",
    "        cutoff_frequency = min(rows, cols) // 3\n",
    "\n",
    "        high_pass_mask = torch.ones_like(x_fft_shifted, dtype=torch.float32)\n",
    "        low_pass_mask = torch.zeros_like(x_fft_shifted, dtype=torch.float32)\n",
    "        \n",
    "        y, x = torch.meshgrid(torch.arange(rows), torch.arange(cols), indexing='ij')\n",
    "        dist = torch.sqrt((x - center_col)**2 + (y - center_row)**2)\n",
    "\n",
    "        high_pass_mask[:, :, dist <= cutoff_frequency] = 0\n",
    "        low_pass_mask[:, :, dist <= cutoff_frequency] = 1\n",
    "\n",
    "        high_pass = x_fft_shifted * high_pass_mask\n",
    "        low_pass = x_fft_shifted * low_pass_mask\n",
    "\n",
    "        high_pass_ifft = torch.fft.ifft2(torch.fft.ifftshift(high_pass, dim=(2, 3)), dim=(2, 3)).real\n",
    "        low_pass_ifft = torch.fft.ifft2(torch.fft.ifftshift(low_pass, dim=(2, 3)), dim=(2, 3)).real\n",
    "        \n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x_1) + self.shortcut(low_pass_ifft))\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block,L_BasicBlock, num_block, num_classes=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        #we use a different inputsize than the original paper\n",
    "        #so conv2_x's stride is 1\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(L_BasicBlock, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(L_BasicBlock, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        intermediate_outputs = {}\n",
    "        output = self.conv1(x)\n",
    "        intermediate_outputs['conv1'] = output\n",
    "        output = self.conv2_x(output)\n",
    "        intermediate_outputs['conv2_x'] = output\n",
    "        output = self.conv3_x(output)\n",
    "        intermediate_outputs['conv3_x'] = output\n",
    "        output = self.conv4_x(output)\n",
    "        intermediate_outputs['conv4_x'] = output\n",
    "        output = self.conv5_x(output)\n",
    "        intermediate_outputs['conv5_x'] = output\n",
    "        output = self.avg_pool(output)\n",
    "        intermediate_outputs['avg_pool'] = output\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "        intermediate_outputs['fc'] = output\n",
    "\n",
    "        return intermediate_outputs\n",
    "\n",
    "def resnet18():\n",
    "    \"\"\" return a ResNet 18 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock,L_BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "def resnet34():\n",
    "    \"\"\" return a ResNet 34 object\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock,L_BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def resnet50():\n",
    "    \"\"\" return a ResNet 50 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck,L_BottleNeck, [3, 4, 6, 3])\n",
    "\n",
    "def resnet101():\n",
    "    \"\"\" return a ResNet 101 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck,L_BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "def resnet152():\n",
    "    \"\"\" return a ResNet 152 object\n",
    "    \"\"\"\n",
    "    return ResNet(BottleNeck, L_BottleNeck, [3, 8, 36, 3])\n",
    "\n",
    "def extract_feature(image,to_do):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, len(to_do) + 1, 1)\n",
    "    plt.imshow(image.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    for i in range(len(to_do)):\n",
    "        feature_map = to_do[i]\n",
    "        mean_feature_map = torch.mean(feature_map, dim=0)  # 채널 방향으로 평균 계산\n",
    "        \n",
    "        plt.subplot(1, len(to_do) + 1, i + 2)\n",
    "        plt.imshow(mean_feature_map.cpu().detach().numpy(), cmap='viridis')\n",
    "        plt.axis('off')\n",
    "\n",
    "# 이미지 전처리 함수 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "# GPU 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "net = resnet18().to(device)\n",
    "net.load_state_dict((torch.load(\"o_BLLB.pth\")))\n",
    "# net.load_state_dict((torch.load(\"original.pth\")))\n",
    "net.eval()\n",
    "\n",
    "\n",
    "data_iter = iter(testloader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# 배치 중 첫 번째 이미지를 선택합니다. (9,45,39,54,57)\n",
    "image = images[9].unsqueeze(0).to(device)  # 배치 차원 추가\n",
    "# Get the intermediate outputs\n",
    "with torch.no_grad():\n",
    "    intermediate_outputs = net(image)\n",
    "\n",
    "\n",
    "\n",
    "to_do = []\n",
    "\n",
    "# # Access intermediate feature maps\n",
    "conv1_features = intermediate_outputs['conv1']\n",
    "conv2_x_features = intermediate_outputs['conv2_x']\n",
    "conv3_x_features = intermediate_outputs['conv3_x']\n",
    "conv4_x_features = intermediate_outputs['conv4_x']\n",
    "conv5_x_features = intermediate_outputs['conv5_x']\n",
    "\n",
    "\n",
    "to_do.extend([\n",
    "    conv1_features[0],\n",
    "    conv2_x_features[0],\n",
    "    conv3_x_features[0],\n",
    "    conv4_x_features[0],\n",
    "    conv5_x_features[0],\n",
    "])\n",
    "\n",
    "\n",
    "extract_feature(image,to_do)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
